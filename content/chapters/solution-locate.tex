\chapter{The \textit{Locate} step}
\label{chap:locate}

\newcommand{\locateimgsize}{0.9\textwidth}

The task of the \textit{Locate} step consists in extracting the positions of the bubbles in pixel coordinates, independently for each frame of each camera.

\section{State of the art}

When searching in literature, many approaches were found to tackle the \textit{Locate} problem.
They were tested and compared with the algorithmic ideas developed within this thesis:
\begin{itemize}
	\itemsep 0em
	\item Section~\ref{sec:locate:trackpy} explores the Trackpy~\cite{trackpy} Python library;
	\item Section~\ref{sec:locate:myptv} explores the MyPTV~\cite{myptv} Python library;
	\item Section~\ref{sec:locate:tractrac} explores the TracTrac~\cite{tractrac} Python program;
	\item Section~\ref{sec:locate:fourdptv} explores the 4d-ptv~\cite{fourdptv} Matlab script.
\end{itemize}

\section{Requirements}

\subsection{Input}
The \textit{Locate} step receives as input the videos captured by the cameras, frame by frame.
The images are pre-processed by an FPGA included in the cameras: the background is removed, and the resulting image is binarized with a threshold, to have distinct white bubbles over a black background.
Figure~\ref{fig:locate:original} depicts an example input frame.

\begin{figure}
	\centerline{\includegraphics[width=\locateimgsize]{images/locate/_original-frame-full.png}}
	\caption{\centering An example of frame returned by the cameras}
	\label{fig:locate:original}
\end{figure}

\subsection{Output}

The output of the \textit{Locate} step is a couple of \texttt{numpy} arrays.
An array called \texttt{positions} describes the coordinates of each bubble present in a frame. It is a four-dimensional, floating-point array, where \texttt{positions[C][F][B]} describes the $B$-th bubble of the $F$-th frame of camera $C$, in the form of an \texttt{(x, y[, area])} tuple.
Bubbles are ordered in a random, arbitrary way.

Due to \texttt{numpy} limitations, the array is pre-allocated of a fixed size: while the number of cameras is fixed, an upper limit on the number of frames and bubbles must be decided before execution.
Knowing which frames contain meaningful data is trivial, while it is not for the number of bubbles, that can change between frame and frame.
For this reason, a second array was introduced: \texttt{numTracers} is a two-dimensional, integer array.
\texttt{numTracers[C][F]} carries the information of how many tracers are valid inside the $F$-th frame of camera $C$.
The coordinates of the valid tracers will therefore be \texttt{positions[C][F][numTracers[C][F]]}.

\subsection{Speed}

When used on a setup of $N$ cameras with frame rate $f$ each, the \textit{Locate} step would receive $N{\cdot}f$ independent frames each second.
To respect the real-time constraint, the \textit{Locate} step would therefore need to operate at a minimum of $N{\cdot}f$ FPS.

When I did the analysis on the \textit{Locate} step, the plan was to have 3 cameras working at 30 FPS, requiring a 90 FPS \textit{Locate} step.
Later, the cameras turned out to be slower, at 24 FPS, but there were 4: the final \textit{Locate} implementation was able to manage also these 96 FPS.

\subsection{Quality}

Ideally, all tracers should be detected, since errors in the locating process would propagate to future steps:
\begin{itemize}
	\itemsep 0em
	\item \textbf{False positives}: the \textit{3D matching} phase will have more candidates, leading to \textit{possible} wrong reconstructions: the \textit{3D matching} can both choose the correct bubble, or the one added by the error (or another real one);
	\item \textbf{False negatives:} the same bubble in another frame will not have the correct match, leading to \textit{certain} wrong reconstructions.
\end{itemize}
As such, it is better to overpredict (false positives) than to miss bubbles.

It is however to be noted that the most important requirement is the speed: a worse implementation which is speedwise above target should be preferred to a better implementation that does not meet speed requirements.

\section{Approaches}

The following sections describe the many different approaches evaluated for the \textit{Locate} step.
Their speed and quality is compared on a common 1-camera, 100-frame sequence.
Each approach reports an example frame: it is the result of performing the \textit{Locate} on figure~\ref{fig:locate:original-crop}, which itself is a portion of the frame in figure~\ref{fig:locate:original}.

\begin{figure}[H]
	\centerline{\includegraphics[width=\locateimgsize]{images/locate/_original-frame.png}}
	\caption{\centering An example of frame returned by the cameras}
	\label{fig:locate:original-crop}
\end{figure}


\newpage
\input{content/chapters/solution-locate/trackpy.tex} \newpage
\input{content/chapters/solution-locate/trackpy-cupy.tex} \newpage
\input{content/chapters/solution-locate/CNN.tex} \newpage
\input{content/chapters/solution-locate/torch.unfold.tex} \newpage
\input{content/chapters/solution-locate/myptv.tex} \newpage
\input{content/chapters/solution-locate/gpu-algorithm.tex} \newpage
\input{content/chapters/solution-locate/iterative.tex} \newpage
\input{content/chapters/solution-locate/tractrac.tex} \newpage
\input{content/chapters/solution-locate/hough.tex} \newpage
\input{content/chapters/solution-locate/hough-gpu.tex} \newpage
\input{content/chapters/solution-locate/4dptv.tex} \newpage
\input{content/chapters/solution-locate/moments.tex} \newpage
\input{content/chapters/solution-locate/moments-gpu.tex} \newpage

\section{Final choice}


% \begin{figure}
% 	\centerline{\includegraphics[width=.6\textwidth]{images/moments_opencv_vs_gpu.png}}
% 	\caption{\centering Comparing speed between CPU and GPU Image Moments algorithms with respect to number of frames processed.}
% 	\label{fig:locate:moments-cpu-vs-gpu}
% \end{figure}
