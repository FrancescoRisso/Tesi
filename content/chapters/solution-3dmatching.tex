\chapter{The \match* step}
\label{chap:matching}

The \match* step has the objective of leveraging the different camera views to estimate the depth (and, consequently, the 3D coordinates) of the bubbles.
The depth estimation is done with the technique of stereoscopy (explained in section~\ref{sec:backgr:stereo}).
In particular, the \match* step needs to match the same bubble across different cameras, to then call \texttt{OpenCV}'s \texttt{triangulatePoints} function for reconstructing the 3D positions.

\section{Requirements}

\subsection{Input}

Depending on the pipeline order, the input to the \match* step can be either the output of the \locate* step or the \linkDD* step.
In both cases, the input is composed of two arrays:
\begin{itemize}
	\itemsep 0em
	\item the array of the coordinates \texttt{positions[C][F][B]} (described in sections~\ref{sec:locate:output} and \ref{sec:linkDD:output}), which has the same format in both cases;
	\item the representation of valid coordinates, which varies based on the previous step: \locate* has a \texttt{numTracers[C][F]}, as explained in section~\ref{sec:locate:output}, while \linkDD* uses a larger \texttt{validTracers[C][F][B]}, described in section~\ref{sec:linkDD:output}.
\end{itemize}
In both cases, the \match* operates on the coordinates of the valid 2D coordinates: an initial step extracts such coordinates from the \texttt{positions} array, leveraging the other array in the correct way.
After that, the \match* can be executed without caring about the source of the data.

\subsection{Output}
\label{sec:match:output}

The \match* step's output consists in a couple of arrays, similar to the output of the \linkDD* (described in section~\ref{sec:linkDD:output}).

The 3D coordinates of the bubbles are stored in a three-dimensional, floating-point array called \texttt{positions}. \texttt{positions[F][B]} contains the coordinates of the $B$-th bubble in the $F$-th time instant, as a tuple \texttt{(x, y, z)}.
The camera index disappeared, since this step combines the information from all cameras into a single, 3D description of the bubbles.

To represent the valid bubbles, a three-dimensional, boolean \texttt{validTracers} array is used.
\texttt{validTracers[F][B]} marks whether \texttt{positions[F][B]} contains a valid position or not.
Similarly to the \texttt{positions} array, the passage from 2D to 3D removes the dimension of the camera index.

Depending on the source of data, this step can either produce plain 3D coordinates, or 3D tracklets.
If the input is the \locate* step, then the bubble coordinates will be all grouped towards smaller $B$s, and there will be no correlation between bubbles with the same index in consecutive frames.
Instead, if the input is the \linkDD* step, the distribution of real coordinates within the $B$ dimension of the arrays will be less regular, due to the added constraint that ``same value for $B$ implies same real bubble''.

\subsection{Speed}

To respect the real time constraint, the \match* step should operate at 30 FPS.

\subsection{Quality}

The matching algorithms start from a bubble in one ``main'' camera, and try to find the matching bubble on the other cameras.
Such attempt can have three outcomes:
\begin{itemize}
	\itemsep 0em
	\item correct match: the bubble is matched to the correct one in the other camera. This is the ideal case, since it would lead to a correct 3D reconstruction;
	\item missing match: the original bubble is not matched to another one. Since the setup is composed by more than 2 cameras, a missed match is not too terrible, since it is still possible that the matches in the other cameras are correct, to have a correct 3D reconstruction;
	\item wrong match: the bubble is matched to a wrong one. This leads to certain reconstruction errors, since the \texttt{triangulatePoints} function will have either wrong or incoherent information.
\end{itemize}
The ideal matching algorithm is therefore one that produces correct matches for all bubbles, but if it's not possible, it's better to have missing matches rather than wrong matches.

\section{State of the art}

For the \match*, no existing solution was found when looking on the Internet: the matching is usually done on full images, not on single sets of coordinates.
The only starting point available for this step was the original tool developed by the research group, the objective of the acceleration.
In particular, it allows to choose between an epiline-only approach (examined in section~\ref{sec:match:epiline}) and a brute force algorithm (evaluated in section~\ref{sec:match:bruteforce}).

When exploring the Trackpy~\cite{trackpy} documentation, the phrase ``Trackpy is a Python package for particle tracking in 2D, 3D'' may lead to think that Trackpy also performs the \match* task.
Contrarily, this means that Trackpy is able to perform tracking with 3D data in input, collected for instance using the confocal microscopy technique.
As such, it is not a useful approach for this step.

\section{Approaches}

The following chapters describe the different approaches explored for the \match* step
The evaluation was performed separately for speed and quality, with the speed measured on the same dataset as the \linkDD* step.
For the quality, it was not possible to have a ground truth containing the correct match between two cameras, neither with a real dataset nor with a synthetic one.
As such, a manual classification of correct/missed/wrong matches was done.
As visible in figure~\ref{fig:match:example}, the displacement between the view of two cameras is mostly constant: checking this consistency allows a human eye to evaluate the correctness of a match.

The qualitative evaluation was performed using two Blender-generated datasets, composed by a single frame, with respectively 30 and 100 bubbles.
An attempt was done with more bubbles, but the image was too crowded to identify correct or wrong matches.

\begin{figure}[H]
	\centerline{\includegraphics[width=0.8\textwidth]{images/match-observation.png}}
	\caption{\centering An example of \match*: the bubbles seen by the main (white) camera, matched to the corresponding bubbles seen from the other two (red, green) cameras}
	\label{fig:match:example}
\end{figure}

\newpage
\input{content/chapters/solution-match/traj.tex} \newpage
\input{content/chapters/solution-match/epilines.tex} \newpage
\input{content/chapters/solution-match/epilines-median.tex} \newpage
\input{content/chapters/solution-match/epilines-traj.tex} \newpage
\input{content/chapters/solution-match/epilines-knn.tex} \newpage
\input{content/chapters/solution-match/bruteforce.tex} \newpage
\input{content/chapters/solution-match/bruteforce-epilines.tex} \newpage

\section{Final choice}

Figure~\ref{fig:match:comparison} compares speed and quality of the different approaches.
Due to the real-time constraint, the long trajectories and the brute force algorithms are discarded.
Among the remaining approaches, the median approach is the best one, hence it is the selected one. 

\begin{figure}
	\centerline{\includegraphics[width=0.9\textwidth]{images/3d-matching-comparison-without-chosen.png}}
	\caption{\centering Comparing speed and quality of the various \match* approaches}
	\label{fig:match:comparison}
\end{figure}
